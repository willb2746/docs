---
title: 'Streaming Responses'
description: 'Implement real-time streaming responses with Phonely'
---

# Streaming Responses with Phonely

Streaming responses allow your application to receive and display AI-generated text as it's being created, rather than waiting for the complete response to be finished. This creates a more engaging and natural experience for users, similar to watching someone type in real-time.

## Why Use Streaming?

<CardGroup cols={3}>
  <Card title="Better UX" icon="user">
    Users don't have to wait for complete responses before seeing content
  </Card>
  <Card title="Faster Perceived Speed" icon="bolt">
    First tokens appear quickly, making the app feel more responsive
  </Card>
  <Card title="Progressive UI Updates" icon="display">
    Build interfaces that update incrementally with incoming content
  </Card>
</CardGroup>

## Basic Streaming Implementation

Implementing streaming with Phonely is straightforward - just set the `stream` parameter to `true` in your API request:

<CodeGroup>
```javascript Basic Streaming Example
const fetch = require('node-fetch');

async function streamChatCompletion(userMessage) {
  const response = await fetch('https://fast.phonely.ai/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.PHONELY_API_KEY}`
    },
    body: JSON.stringify({
      model: 'phonely-fast-1',
      messages: [
        {
          role: 'user',
          content: userMessage
        }
      ],
      stream: true
    })
  });

  // Check if the response is successful
  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`API request failed: ${errorText}`);
  }

  // Process the stream
  const reader = response.body.getReader();
  const decoder = new TextDecoder('utf-8');

  let fullResponse = '';

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        break;
      }

      // Decode the stream chunk
      const chunk = decoder.decode(value);
      
      // Split the chunk into lines
      const lines = chunk.split('\n');
      
      // Process each line
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.substring(6);
          
          // Check for the stream end marker
          if (data === '[DONE]') {
            continue;
          }
          
          try {
            const parsedData = JSON.parse(data);
            const content = parsedData.choices[0]?.delta?.content || '';
            
            if (content) {
              // Process each new piece of content as it arrives
              fullResponse += content;
              console.log(content); // In a real app, update UI here
            }
          } catch (error) {
            console.error('Error parsing chunk:', error);
          }
        }
      }
    }
    
    return fullResponse;
  } finally {
    reader.releaseLock();
  }
}

// Example usage
async function main() {
  try {
    const response = await streamChatCompletion("Explain quantum computing in simple terms.");
    console.log("\nFull response:", response);
  } catch (error) {
    console.error("Error:", error);
  }
}

main();
```
</CodeGroup>

## Frontend Implementation

Here's how to implement streaming in a web application:

<CodeGroup>
```javascript React Implementation
import { useState, useEffect, useRef } from 'react';

function ChatInterface() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [streamingMessage, setStreamingMessage] = useState('');
  
  const messagesEndRef = useRef(null);
  
  // Scroll to bottom when messages change
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages, streamingMessage]);
  
  const sendMessage = async (e) => {
    e.preventDefault();
    if (!input.trim()) return;
    
    // Add user message
    setMessages(prev => [...prev, { role: 'user', content: input }]);
    const userInput = input;
    setInput('');
    
    // Start streaming
    setIsLoading(true);
    setStreamingMessage('');
    
    try {
      const response = await fetch('/api/chat-stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ message: userInput }),
      });
      
      if (!response.ok) {
        throw new Error(`API request failed: ${response.statusText}`);
      }
      
      const reader = response.body.getReader();
      const decoder = new TextDecoder('utf-8');
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.substring(6);
            if (data === '[DONE]') continue;
            
            try {
              const parsedData = JSON.parse(data);
              const content = parsedData.choices[0]?.delta?.content || '';
              
              if (content) {
                setStreamingMessage(prev => prev + content);
              }
            } catch (error) {
              console.error('Error parsing chunk:', error);
            }
          }
        }
      }
      
      // After streaming is complete, add the full message to the messages array
      setMessages(prev => [...prev, { role: 'assistant', content: streamingMessage }]);
      setStreamingMessage('');
      
    } catch (error) {
      console.error('Error:', error);
      setMessages(prev => [...prev, { 
        role: 'assistant', 
        content: 'Sorry, something went wrong. Please try again.' 
      }]);
    } finally {
      setIsLoading(false);
    }
  };
  
  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map((msg, index) => (
          <div key={index} className={`message ${msg.role}`}>
            {msg.content}
          </div>
        ))}
        
        {streamingMessage && (
          <div className="message assistant">
            {streamingMessage}
            <span className="cursor"></span>
          </div>
        )}
        
        <div ref={messagesEndRef} />
      </div>
      
      <form onSubmit={sendMessage} className="input-form">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="Type your message..."
          disabled={isLoading}
        />
        <button type="submit" disabled={isLoading}>
          {isLoading ? 'Generating...' : 'Send'}
        </button>
      </form>
    </div>
  );
}

export default ChatInterface;
```

```css Styling (CSS)
.chat-container {
  display: flex;
  flex-direction: column;
  height: 500px;
  width: 100%;
  max-width: 600px;
  border: 1px solid #e0e0e0;
  border-radius: 8px;
  overflow: hidden;
  margin: 0 auto;
}

.messages {
  flex: 1;
  overflow-y: auto;
  padding: 16px;
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.message {
  max-width: 80%;
  padding: 12px 16px;
  border-radius: 18px;
  line-height: 1.4;
}

.user {
  align-self: flex-end;
  background-color: #16A34A;
  color: white;
  border-bottom-right-radius: 4px;
}

.assistant {
  align-self: flex-start;
  background-color: #f0f0f0;
  color: #333;
  border-bottom-left-radius: 4px;
}

.cursor {
  display: inline-block;
  width: 6px;
  height: 16px;
  background-color: #333;
  margin-left: 2px;
  animation: blink 1s infinite;
  vertical-align: middle;
}

@keyframes blink {
  0%, 100% { opacity: 1; }
  50% { opacity: 0; }
}

.input-form {
  display: flex;
  padding: 12px;
  border-top: 1px solid #e0e0e0;
}

input {
  flex: 1;
  padding: 10px 14px;
  border: 1px solid #ddd;
  border-radius: 18px;
  outline: none;
}

button {
  margin-left: 8px;
  padding: 10px 16px;
  background-color: #16A34A;
  color: white;
  border: none;
  border-radius: 18px;
  cursor: pointer;
}

button:disabled {
  background-color: #a0a0a0;
  cursor: not-allowed;
}
```
</CodeGroup>

## Server-Side Implementation

Here's how to implement the server endpoint to handle streaming:

<CodeGroup>
```javascript Node.js Express Server
const express = require('express');
const fetch = require('node-fetch');
const cors = require('cors');
require('dotenv').config();

const app = express();
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// Streaming endpoint
app.post('/api/chat-stream', async (req, res) => {
  const { message } = req.body;
  
  if (!message) {
    return res.status(400).json({ error: 'Message is required' });
  }
  
  try {
    // Set up SSE
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    
    // Forward the request to Phonely
    const phonelyResponse = await fetch('https://fast.phonely.ai/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.PHONELY_API_KEY}`
      },
      body: JSON.stringify({
        model: 'phonely-fast-1',
        messages: [
          {
            role: 'user',
            content: message
          }
        ],
        stream: true
      })
    });
    
    // Check for errors
    if (!phonelyResponse.ok) {
      const errorText = await phonelyResponse.text();
      throw new Error(`Phonely API error: ${errorText}`);
    }
    
    // Stream the response to the client
    const reader = phonelyResponse.body.getReader();
    
    async function streamToClient() {
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) {
            // Send the end marker
            res.write('data: [DONE]\n\n');
            res.end();
            break;
          }
          
          // Forward chunks directly to the client
          const chunk = new TextDecoder().decode(value);
          res.write(chunk);
          // Flush to ensure data is sent immediately
          res.flush?.();
        }
      } catch (error) {
        console.error('Error streaming response:', error);
        res.end();
      }
    }
    
    streamToClient();
    
  } catch (error) {
    console.error('Error:', error);
    // If headers haven't been sent yet, send error response
    if (!res.headersSent) {
      res.status(500).json({ error: 'Something went wrong' });
    } else {
      // Otherwise try to close the stream with an error message
      try {
        res.write('data: {"error": "Stream error occurred"}\n\n');
        res.end();
      } catch (e) {
        res.end();
      }
    }
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

```python Python FastAPI Server
from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import os
import asyncio
from dotenv import load_dotenv

load_dotenv()

app = FastAPI()

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class MessageRequest(BaseModel):
    message: str

@app.post("/api/chat-stream")
async def chat_stream(request: MessageRequest, response: Response):
    # Set up streaming response headers
    response.headers["Content-Type"] = "text/event-stream"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Connection"] = "keep-alive"
    
    # Create an async generator for streaming
    async def event_generator():
        async with httpx.AsyncClient() as client:
            try:
                # Stream response from Phonely API
                async with client.stream(
                    "POST",
                    "https://fast.phonely.ai/openai/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {os.getenv('PHONELY_API_KEY')}"
                    },
                    json={
                        "model": "phonely-fast-1",
                        "messages": [
                            {
                                "role": "user",
                                "content": request.message
                            }
                        ],
                        "stream": True
                    },
                    timeout=60.0
                ) as r:
                    # Forward each chunk to the client
                    async for chunk in r.aiter_bytes():
                        yield chunk
                    
                    # Send end marker
                    yield b"data: [DONE]\n\n"
            
            except Exception as e:
                print(f"Error: {e}")
                error_msg = f"data: {{\"error\": \"{str(e)}\"}}\n\n"
                yield error_msg.encode("utf-8")
    
    # Return the streaming response
    return Response(
        content=event_generator(),
        media_type="text/event-stream"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3000)
```
</CodeGroup>

## Advanced Streaming Features

### Stream Options

Phonely provides additional options to customize streaming behavior:

<CodeGroup>
```javascript Advanced Stream Options
{
  "model": "phonely-fast-1",
  "messages": [
    {
      "role": "user",
      "content": "Write a short story about a space explorer"
    }
  ],
  "stream": true,
  "stream_options": {
    "include_usage": true,    // Include token usage info in the final chunk
    "include_prompt": false,  // Don't stream the prompt tokens
    "chunk_size": 10          // Send chunks of approximately 10 tokens
  }
}
```
</CodeGroup>

### Real-time UI Feedback

You can enhance the streaming experience with real-time UI feedback:

<CodeGroup>
```jsx Typing Animation
function TypingAnimation({ text, speed = 30 }) {
  const [displayedText, setDisplayedText] = useState('');
  const textRef = useRef(text);
  
  useEffect(() => {
    textRef.current = text;
    
    let currentIndex = displayedText.length;
    if (currentIndex >= text.length) return;
    
    // Only animate new text
    const timer = setInterval(() => {
      if (currentIndex < textRef.current.length) {
        setDisplayedText(textRef.current.substring(0, currentIndex + 1));
        currentIndex++;
      } else {
        clearInterval(timer);
      }
    }, speed);
    
    return () => clearInterval(timer);
  }, [text, displayedText, speed]);
  
  return (
    <div className="typing-animation">
      {displayedText}
      {displayedText.length < text.length && <span className="cursor"></span>}
    </div>
  );
}
```

```jsx Code Block Formatting
// Assumes you're using a markdown library like react-markdown
import ReactMarkdown from 'react-markdown';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { tomorrow } from 'react-syntax-highlighter/dist/esm/styles/prism';

function StreamingMessage({ content }) {
  // Detect if we're in the middle of a code block
  const [inProgressCodeBlock, setInProgressCodeBlock] = useState(false);
  const [language, setLanguage] = useState('');
  
  useEffect(() => {
    // Check if we're in the middle of a code block
    const codeBlockStartMatch = content.match(/```(\w*)\n/g);
    const codeBlockEndCount = (content.match(/```\s*$/gm) || []).length;
    
    if (codeBlockStartMatch) {
      const starts = codeBlockStartMatch.length;
      setInProgressCodeBlock(starts > codeBlockEndCount);
      
      // Try to extract language
      if (starts > 0 && codeBlockStartMatch[starts-1]) {
        const langMatch = codeBlockStartMatch[starts-1].match(/```(\w*)\n/);
        setLanguage(langMatch && langMatch[1] ? langMatch[1] : '');
      }
    } else {
      setInProgressCodeBlock(false);
    }
  }, [content]);
  
  return (
    <div className="streaming-message">
      <ReactMarkdown
        components={{
          code({node, inline, className, children, ...props}) {
            const match = /language-(\w+)/.exec(className || '');
            return !inline && match ? (
              <SyntaxHighlighter
                style={tomorrow}
                language={match[1]}
                PreTag="div"
                {...props}
              >
                {String(children).replace(/\n$/, '')}
              </SyntaxHighlighter>
            ) : (
              <code className={className} {...props}>
                {children}
              </code>
            );
          }
        }}
      >
        {content}
      </ReactMarkdown>
      
      {inProgressCodeBlock && (
        <div className="code-in-progress-indicator">
          <span>Writing code in {language || 'progress'}...</span>
        </div>
      )}
    </div>
  );
}
```
</CodeGroup>

## Streaming with Function Calling

You can also use streaming with function calling for more dynamic interactions:

<CodeGroup>
```javascript Streaming with Function Calling
async function streamWithFunctionCalling(userMessage) {
  // First request with function definitions
  const response = await fetch('https://fast.phonely.ai/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.PHONELY_API_KEY}`
    },
    body: JSON.stringify({
      model: 'phonely-fast-1',
      messages: [
        {
          role: 'user',
          content: userMessage
        }
      ],
      tools: [
        // Your function definitions
      ],
      stream: true
    })
  });
  
  // Process stream to detect function calls
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  
  let partialMessage = '';
  let toolCalls = [];
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = decoder.decode(value);
    const lines = chunk.split('\n').filter(line => line.trim() !== '');
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.substring(6);
        if (data === '[DONE]') continue;
        
        try {
          const parsedData = JSON.parse(data);
          
          // Check for tool calls
          if (parsedData.choices[0].delta.tool_calls) {
            // Handle tool call updates
            const toolCallDelta = parsedData.choices[0].delta.tool_calls[0];
            
            if (toolCallDelta.index === 0 && toolCallDelta.id) {
              // New tool call
              toolCalls.push({
                id: toolCallDelta.id,
                function: {
                  name: toolCallDelta.function?.name || '',
                  arguments: toolCallDelta.function?.arguments || ''
                }
              });
            } else if (toolCalls.length > 0) {
              // Update to existing tool call
              const lastToolCall = toolCalls[toolCalls.length - 1];
              
              if (toolCallDelta.function?.name) {
                lastToolCall.function.name += toolCallDelta.function.name;
              }
              
              if (toolCallDelta.function?.arguments) {
                lastToolCall.function.arguments += toolCallDelta.function.arguments;
              }
            }
          } else if (parsedData.choices[0].delta.content) {
            // Regular content updates
            const content = parsedData.choices[0].delta.content;
            partialMessage += content;
            
            // Update UI with content
            console.log(content);
          }
        } catch (error) {
          console.error('Error parsing chunk:', error);
        }
      }
    }
  }
  
  // If we detected tool calls, execute them and stream the final response
  if (toolCalls.length > 0) {
    console.log('Executing tool calls:', toolCalls);
    
    // Execute the tool calls and get results
    const toolResults = await Promise.all(toolCalls.map(async (toolCall) => {
      const args = JSON.parse(toolCall.function.arguments);
      // Execute the appropriate function based on name
      // ...
      return {
        tool_call_id: toolCall.id,
        role: 'tool',
        content: JSON.stringify({ result: 'mock result' }) // Replace with actual result
      };
    }));
    
    // Stream the final response with tool results
    // ...
  }
  
  return partialMessage;
}
```
</CodeGroup>

## Performance Considerations

<AccordionGroup>
  <Accordion title="Server-Side Performance">
    Streaming requires keeping a connection open longer. Ensure your server is configured to handle long-lived connections and has appropriate timeouts.
  </Accordion>
  <Accordion title="Frontend Performance">
    DOM updates for each token can be expensive. Consider batching updates or using virtual DOM libraries like React for efficient rendering.
  </Accordion>
  <Accordion title="Error Handling">
    Implement robust error recovery for stream interruptions, including reconnection strategies and error messages.
  </Accordion>
  <Accordion title="Progressive Enhancement">
    Provide a fallback for environments that don't support streaming by detecting support and falling back to non-streaming requests.
  </Accordion>
</AccordionGroup>

## Best Practices

<Steps>
  <Step title="Provide immediate feedback">
    Show a loading indicator or typing animation as soon as the user sends a message.
  </Step>
  <Step title="Buffer short responses">
    For very short responses, consider buffering the first few tokens to avoid choppy display.
  </Step>
  <Step title="Handle network issues">
    Implement reconnection logic if the stream connection is interrupted.
  </Step>
  <Step title="Consider UX for code and markdown">
    Special handling for code blocks and markdown formatting can improve the user experience during streaming.
  </Step>
</Steps>

By implementing streaming responses, you'll create a more natural and engaging experience for users of your Phonely-powered applications, making conversations feel more like real-time human interactions. 